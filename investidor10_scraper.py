# investidor10_scraper.py
# Autor: ChatGPT para S칠rgio Gaigher
# Objetivo: Capturar dados da carteira p칰blica no Investidor10 em hor치rios definidos, com logs detalhados e screenshot para debugging

import asyncio
from datetime import datetime
from pathlib import Path
from playwright.async_api import async_playwright
import json
import sys

print("游댠 SCRIPT INVESTIDOR10_INICIADO")


# URL da carteira
URL = "https://investidor10.com.br/carteira/545535/"

# Caminhos para salvar dados conforme o hor치rio
ARQUIVOS = {
    "00:00": "data/crypto_open.json",
    "10:30": "data/open_prices.json",
    "17:00": "data/final_prices.json"
}

async def extrair_dados(horario_execucao):
    print(f"游뎷 Entrou na fun칞칚o com hor치rio: {horario_execucao}")

    print(f"[IN칈CIO] Hor치rio: {horario_execucao} | Data/Hora: {datetime.now()}")

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        await page.goto(URL, timeout=60000)

        # Espera seletor e d치 tempo extra para o JS renderizar
        await page.wait_for_selector(".table-responsive", timeout=60000)
        await page.wait_for_timeout(5000)

        # Cria pasta se n칚o existir
        Path("data").mkdir(parents=True, exist_ok=True)

        # Salva screenshot para debug
        screenshot_path = f"data/screenshot_{horario_execucao.replace(':','')}.png"
        await page.screenshot(path=screenshot_path, full_page=True)
        print(f"[九] Screenshot salva em: {screenshot_path}")

        # Captura HTML bruto da p치gina
        conteudo = await page.content()
        html_path = f"data/carteira_{horario_execucao.replace(':','')}.html"
        with open(html_path, "w", encoding="utf-8") as f:
            f.write(conteudo)
        print(f"[九] HTML bruto salvo: {html_path}")

        # Captura conte칰do das tabelas
        tabelas = await page.query_selector_all(".table-responsive")
        print(f"[INFO] Quantidade de tabelas capturadas: {len(tabelas)}")

        dados = []
        for idx, tabela in enumerate(tabelas):
            html = await tabela.inner_html()
            dados.append({"index": idx, "html": html})

        await browser.close()

        # Salva JSON com dados HTML das tabelas
        destino_json = ARQUIVOS.get(horario_execucao)
        if destino_json:
            with open(destino_json, "w", encoding="utf-8") as f:
                json.dump({"data": str(datetime.now()), "tabelas": dados}, f, ensure_ascii=False, indent=2)
            print(f"[九] Dados extra칤dos salvos em: {destino_json}")
        else:
            print("[丘] Hor치rio inv치lido fornecido. Nenhum JSON salvo.")

if __name__ == "__main__":
    try:
        if len(sys.argv) > 1:
            horario = sys.argv[1]
            print(f"[ARGS] Hor치rio recebido via argumento: {horario}")
        else:
            horario = "17:00"
            print("[ARGS] Nenhum argumento recebido, usando default: 17:00")

        print(f"[INFO] Executando script para hor치rio: {horario}")
        asyncio.run(extrair_dados(horario))
        print("[INFO] Script finalizado com sucesso.")
    except Exception as e:
        print(f"[ERRO] Falha na execu칞칚o: {str(e)}")
